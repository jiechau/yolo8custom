<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YOLOv8n ONNX Inference</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.10.0/dist/ort.min.js"></script>
    <style>
        body { display: flex; flex-direction: column; align-items: center; }
        video { border: 1px solid black; }
        canvas { position: absolute; top: 0; left: 0; }
    </style>
</head>
<body>
    <h1>YOLOv8n ONNX Inference</h1>
    <video id="video" width="640" height="480" autoplay></video>
    <canvas id="canvas" width="640" height="480"></canvas>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');

        // Load ONNX model
        let session;
        async function loadModel() {
            try {
                session = await ort.InferenceSession.create('yolov8n.onnx', {
                    executionProviders: ['wasm'],
                    wasmPaths: {
                        'ort-wasm.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm.wasm',
                        'ort-wasm-simd.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm-simd.wasm',
                        'ort-wasm-threaded.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm-threaded.wasm',
                        'ort-wasm-simd-threaded.wasm': 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm-simd-threaded.wasm'
                    }
                });
                console.log('ONNX model loaded');
            } catch (error) {
                console.error('Error loading ONNX model:', error);
                alert('Failed to load ONNX model. Please check the console for more details.');
            }
        }

        // Start video stream
        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
            } catch (error) {
                console.error('Error accessing the camera:', error);
                alert('Failed to access the camera. Please check the console for more details.');
            }
        }

        // Perform inference
        async function runInference() {
            try {
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

                // Preprocess image data for ONNX model
                const tensor = tf.browser.fromPixels(imageData)
                    .resizeBilinear([640, 640])
                    .expandDims(0)
                    .toFloat()
                    .div(tf.scalar(255.0));

                const inputTensor = new ort.Tensor('float32', tensor.dataSync(), [1, 3, 640, 640]);

                // Run inference
                const results = await session.run({ input: inputTensor });

                // Process and draw results
                drawResults(results);
            } catch (error) {
                console.error('Error during inference:', error);
                alert('Failed during inference. Please check the console for more details.');
            }
        }

        // Draw results on canvas
        function drawResults(results) {
            // Clear previous drawings
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Draw current frame from video
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            // TODO: Process results and draw bounding boxes on the canvas
            // Example: ctx.strokeRect(x, y, width, height);
        }

        // Initialize
        async function init() {
            try {
                await loadModel();
                await startVideo();
                video.addEventListener('play', () => {
                    setInterval(runInference, 100); // Run inference every 100ms
                });
            } catch (error) {
                console.error('Error during initialization:', error);
                alert('Initialization failed. Please check the console for more details.');
            }
        }

        init();
    </script>
</body>
</html>
